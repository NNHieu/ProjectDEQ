# @package _global_

# to execute this experiment run:
# python run.py experiment=fixedpoint.yaml

defaults:
  - override /mode: exp.yaml
  - override /trainer: default.yaml
  - override /model: fixedpoint.yaml
  - override /datamodule: mnist.yaml
  - override /callbacks: null
  - override /logger: null

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# it's also accessed by loggers
name: "fp${model.arch.h_features}_${model.arch.f_solver}_${model.noise_sd}_${seed}_v02"

seed: 12345

model:
  arch:
    h_features:
    - 16
    - 32
    f_solver: anderson
    f_thres: 30
    b_solver: anderson
    b_thres: 30
  
  lr: 1e-3
  lr_schedule:
    - 120
    - 280
  lr_decay: step
  lr_factor: 0.2
  optimizer_name: adam
  compute_jac_loss: False
  spectral_radius_mode: False
  noise_sd: 0.0

datamodule:
  train_batch_size: 32
  test_batch_size: 32

trainer:
  min_epochs: 1
  max_epochs: 300
  gradient_clip_val: 10 # Follow from https://openreview.net/pdf?id=y1PXylgrXZ

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/acc"
    mode: "max"
    save_top_k: 4
    save_last: True
    verbose: False
    dirpath: "checkpoints/"
    filename: "epoch_{epoch:03d}"
    auto_insert_metric_name: False
  rich_progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar
